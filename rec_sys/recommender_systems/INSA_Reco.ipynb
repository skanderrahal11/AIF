{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVNtZAJ7wZDh"
      },
      "source": [
        "In this practical session, you will implement different strategies to build a recommender system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkXGEpy55hXT"
      },
      "source": [
        "# Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I71b3ARWZId3"
      },
      "source": [
        "We will be utilizing 'The Movie Dataset' hosted on Kaggle, featuring comprehensive metadata for 45,000 movies listed in the Full MovieLens Dataset (accessible here: [Full MovieLens Dataset](https://grouplens.org/datasets/movielens/)). The dataset encompasses films released up until July 2017 and includes a variety of data points such as cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, as well as TMDB vote counts and vote averages.\n",
        "\n",
        "Additionally, this dataset encompasses 26 million ratings from 270,000 users across all 45,000 movies. These ratings, ranging from 1 to 5, are sourced directly from the official GroupLens website.\n",
        "\n",
        "To access this dataset, a [Kaggle](https://www.kaggle.com/) account is required. After logging into Kaggle, navigate to your account settings and scroll to the API section to generate a new API token. This token will facilitate the direct download of the dataset via the Kaggle library in your notebook.\n",
        "\n",
        "![Instructions for generating a Kaggle API token](https://drive.google.com/uc?export=view&id=1YcSTHD_FGrwDKaaLk6T9Gsdte8TKuPCt)\n",
        "\n",
        "Now, you can proceed to install the Kaggle library in your notebook, which allows for the dataset to be downloaded directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czuGtQuDRTIM"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq0iGNLdca65"
      },
      "source": [
        "Run the next cell to upload your token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4O_t6ZDRza5"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqHM7r0_cmhe"
      },
      "source": [
        "We will begin by analyzing the metadata dataset, which includes comprehensive information about the movies.  \n",
        "This dataset encompasses details such as movie titles, descriptions, genres, and even their average IMDb ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUPskNNGRe6M"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f movies_metadata.csv\n",
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f ratings.csv\n",
        "!unzip movies_metadata.csv.zip\n",
        "!unzip ratings.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xptfFMZkcxwg"
      },
      "source": [
        "Use pandas to explore the `movies_metadata.csv` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzn1PFZbFZ10"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "metadata = pd.read_csv('movies_metadata.csv')\n",
        "metadata.dropna(subset=['title'], inplace=True)\n",
        "metadata['id'] = pd.to_numeric(metadata['id'])\n",
        "metadata['genres'] = metadata['genres'].apply(lambda x: [i['name'] for i in eval(x)])\n",
        "metadata = metadata[['id', 'title', 'genres', 'release_date', 'vote_average', 'vote_count']]\n",
        "metadata.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxrwsdTstfHD"
      },
      "source": [
        "Using the release date, create a new column called `year` and use seaborn to plot the number of movies per year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X6vms1Tt5T8"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metadata['year'] = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWUNjSB5oo-"
      },
      "source": [
        "# Recommendation by popularity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a straightforward approach to generating recommendations by suggesting the top 5 movies based on their average score. However, it's worth noting that while average score is a useful metric, it may not always provide the most personalized recommendations, as individual preferences can vary widely."
      ],
      "metadata": {
        "id": "u8I-zN7-Q10X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMeug2Ns_eWR"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSAhfINJuSQr"
      },
      "source": [
        "Have you seen any of these movies?  \n",
        "You may have guessed that the average score is only reliable when it is averaged on a sufficient number of votes.\n",
        "Use seaborn ```histplot``` method to plot the histogram of the number of votes.\n",
        "For better readability you may first do this plot for the movies with less than 100 votes and then do another one for the remaining ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5_OvQG7p_oa"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWmLm-0c_7k-"
      },
      "source": [
        "Try to visualize the best movies according to the average vote for movies that have at least 1000 votes.  \n",
        "You should now know some of these movies now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oie1uXOt_rx9"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-KcoY_Y__TE"
      },
      "source": [
        "## Best movies by IMDb score\n",
        "IMDb (an acronym for Internet Movie Database) is an online database of information related to films, television programs, home videos, video games, and streaming content online.\n",
        "It might be considered as one of the most exhaustive databases on movies.\n",
        "In addition, IMDb maintains a ranking of movies according to people's votes. To do so, it computes a score based on the average rating and the number of votes.\n",
        "The formula they are using is described [here](https://help.imdb.com/article/imdb/track-movies-tv/ratings-faq/G67Y87TFYYP6TWAV#)\n",
        "![](https://drive.google.com/uc?export=view&id=12J_uJ86eOimr8Y0LHTGSMmUgkBnZu9cO)  \n",
        "Use this formula to compute the IMDb score for all movies and visualize the ones with the best scores. (You may use a smaller value for m, 500 for example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HC4ICpX9rMi"
      },
      "outputs": [],
      "source": [
        "m = 500\n",
        "c = metadata.vote_average.mean()\n",
        "\n",
        "def imdb_score(x):\n",
        "    ...\n",
        "\n",
        "metadata['imdb_score'] = metadata.apply(imdb_score, axis=1)\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vEAlQDAu-EM"
      },
      "source": [
        "What were the best movies in your birth year?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzEhMDWqvPsJ"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_sgJyVAUPKT"
      },
      "source": [
        "One way to refine recommendations is by considering popularity and recommending products from the same category or genre. This approach can help ensure that the suggestions are not only highly rated but also aligned with the user's interests.  \n",
        "The following code will create a data frame containing one-hot encoding of the movie's genre.\n",
        "Use it to recommend the best movies according to the genre and the IMDB score (for example the best Horror movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW0S-8DCOlqg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "genre_df = pd.DataFrame(mlb.fit_transform(metadata['genres'].fillna('[]')),columns=mlb.classes_, index=metadata.index)\n",
        "metadata = pd.concat([metadata.drop(columns=['genres']), genre_df], axis=1)\n",
        "metadata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommend some horror or comedy movies."
      ],
      "metadata": {
        "id": "P9W1DbdFkVcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHol9fYDTLGQ"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouHxUzDPUbwb"
      },
      "source": [
        "# Content based recommender systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXdh1FPrWqOh"
      },
      "source": [
        "\n",
        "### Item description\n",
        "An alternative approach to building a recommender system is to generate recommendations based on content characteristics. This method is particularly appealing when dealing with situations where there are limited user interactions or a scarcity of new items to recommend. It's highly probable that your catalog includes supplementary data related to the items. This additional information is typically curated manually and can contain valuable features for the development of a content-based recommender system.\n",
        "\n",
        "In our specific case, we will obtain a dataset that includes keywords associated with the movies to facilitate this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTWzBug47LTR"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download \"rounakbanik/the-movies-dataset\" -f keywords.csv\n",
        "!unzip keywords.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVI8CAsK65h6"
      },
      "outputs": [],
      "source": [
        "keywords = pd.read_csv('keywords.csv')\n",
        "keywords['keywords'] = keywords['keywords'].apply(lambda x: \" \".join([i['name'] for i in eval(x)])).fillna('')\n",
        "keywords.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWT4RXIMB5hd"
      },
      "source": [
        "These keywords can serve as excellent descriptors for our movies.  \n",
        "To enable similarity calculations between movies, it's essential to represent them in a vectorized format.  \n",
        "We will now create another dataframe containing all the movies attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej97GHDtWUel"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "mlb = CountVectorizer(max_features=400)\n",
        "keywords_transformed = mlb.fit_transform(keywords['keywords'])\n",
        "keywords_transformed = pd.DataFrame(keywords_transformed.toarray(), columns=mlb.get_feature_names_out(), index=keywords.index)\n",
        "keywords = pd.concat([keywords.drop(columns=['keywords']), keywords_transformed], axis=1)\n",
        "keywords.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1cuVvCvqwWZ"
      },
      "source": [
        "We will proceed by constructing a dataframe in which each movie is depicted through its attribute vectors, encompassing keywords and genres.  \n",
        "To optimize computational resources and reduce memory requirements, we will restrict our analysis to movies released after the year 2010."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RySZnFB64W-"
      },
      "outputs": [],
      "source": [
        "subset = metadata[metadata['release_date'] > \"2010\"].reset_index(drop=True)\n",
        "\n",
        "attributes_df = pd.merge(subset.drop(columns=['release_date', 'vote_average', 'vote_count', 'year',\n",
        "       'imdb_score']), keywords, on='id')\n",
        "attributes_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAyFzBMhXOe6"
      },
      "source": [
        "Now that you have a representation computed for each movie, you can calculate distances or similarities for movie pairs.  \n",
        "Use scikit-learn 's [cosine_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_distances.html) function, to compute the cosine similarity matrix of your dataframe.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH57BkaU_NlO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "cosine_sim = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZpbREM3Xjp9"
      },
      "source": [
        "Use the following function with your similarity matrix to recommend movies from another movie title.  \n",
        "Try on several movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G-wd7OeGCAd"
      },
      "outputs": [],
      "source": [
        "titles = attributes_df['title']\n",
        "indices = pd.Series(attributes_df.index, index=attributes_df['title'])\n",
        "\n",
        "def get_reco(title, sim_matrix):\n",
        "  idx = indices[title]\n",
        "  print(f'original: {title}')\n",
        "  recos = sim_matrix[idx].argsort()[1:6]\n",
        "  recos = titles.iloc[recos]\n",
        "\n",
        "  print(recos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Kc92Bymf64"
      },
      "outputs": [],
      "source": [
        "title = 'LEGO: The Adventures of Clutch Powers'\n",
        "get_reco(title, cosine_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGPMmv29k8Ii"
      },
      "outputs": [],
      "source": [
        "title = 'Iron Man 2'\n",
        "get_reco(title, cosine_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8VZCO6RlG7A"
      },
      "outputs": [],
      "source": [
        "title = 'How to Train Your Dragon'\n",
        "get_reco(title, cosine_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r8eClr-lc-r"
      },
      "outputs": [],
      "source": [
        "title = 'Alice in Wonderland'\n",
        "get_reco(title, cosine_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGv6FOEkwvvy"
      },
      "source": [
        "Let's free some memory in the Colab instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KMMPluZAM8O"
      },
      "outputs": [],
      "source": [
        "del(cosine_distances)\n",
        "del(attributes_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hpHJS-7Ph9b"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cURcQU4Yt1C"
      },
      "source": [
        "Images are another type of content associated to products.  \n",
        "It may not necessarily be relevant in the case of movies but let's do it anyway.  \n",
        "We will now recommend movies according to their posters.  \n",
        "First, we need to download another dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcR_BXDOkebb"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download \"ghrzarea/movielens-20m-posters-for-machine-learning\"\n",
        "!unzip movielens-20m-posters-for-machine-learning.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbqtGu8zZVIV"
      },
      "source": [
        "The following code will help loading the data with Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slLm9ty3uBA_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "\n",
        "class ImageAndPathsDataset(datasets.ImageFolder):\n",
        "    def __getitem__(self, index):\n",
        "        img, _= super(ImageAndPathsDataset, self).__getitem__(index)\n",
        "        path = self.imgs[index][0]\n",
        "        return img, path\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBfjtzoAaIPf"
      },
      "source": [
        "We will use a pre-trained network to extract the features from the posters.\n",
        "Similar to what we did with the item descriptions, we will compute similarities between the movies according to these features.\n",
        "\n",
        "The pre-trained model was trained with normalized images. Thus, we have to normalize our posters before feeding them to the network.\n",
        "The following code instantiates a data loader with normalized images and provides a function to revert the normalization for visualization purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8RDKcuaozEg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "mean = [ 0.485, 0.456, 0.406 ]\n",
        "std = [ 0.229, 0.224, 0.225 ]\n",
        "normalize = transforms.Normalize(mean, std)\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(mean, std)],\n",
        "   std= [1/s for s in std]\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                normalize])\n",
        "dataset = ImageAndPathsDataset('MLP-20M', transform)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=128, num_workers=2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ-p-y7waHqN"
      },
      "source": [
        "Here are some exemples of posters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEaN1FHVnwtW"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "x, paths = next(iter(dataloader))\n",
        "img_grid = make_grid(x[:16])\n",
        "img_grid = inv_normalize(img_grid)\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(img_grid.permute(1, 2, 0))\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HylAtoxarsu"
      },
      "source": [
        "Instantiate a pre-trained a mobilenet_v3_small model (documentation [here](https://pytorch.org/vision/stable/models.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AihOUOapv-3"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "mobilenet = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLj8Pu3ca4Jb"
      },
      "source": [
        "Have a look to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8Fol_cxa0Ev"
      },
      "outputs": [],
      "source": [
        "mobilenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dGZvEizbA3l"
      },
      "source": [
        "We will now crate a subset of this model to extract the features.\n",
        "Use a Sequential model to get only the features followed by the avgpool layer of mobilnet and finish with a Flatten layer (```torch.nn.Flatten()```)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TMyJ4p6avz5"
      },
      "outputs": [],
      "source": [
        "model = ... .cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGLFcAbbzE8"
      },
      "source": [
        "If your model is OK, it should predict 576-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx6Ue-sHqSM5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x = torch.zeros(100, 3, 224,224).cuda()\n",
        "y = model(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezj6kEcFcX4H"
      },
      "source": [
        "\n",
        "We will proceed to establish a dataframe containing our extracted features, along with the file paths to the poster images.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuVIQdtOI9Y6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "features_list = []\n",
        "paths_list = []\n",
        "\n",
        "for x, paths in tqdm(dataloader):\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(x.cuda())\n",
        "        features_list.extend(embeddings.cpu().numpy())\n",
        "        paths_list.extend(paths)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'features': features_list,\n",
        "    'path': paths_list\n",
        "})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O42UYqDryUhm"
      },
      "source": [
        "We will now extract all the features into a numpy array that will be used to compute the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYS2v2Gn_cJl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "features = np.vstack(features_list)\n",
        "features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOi3Jv0hdWE1"
      },
      "source": [
        "Now compute the cosine similarities between the embeddings of your movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E_dnAlL7cUH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "cosine_sim = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5v_SZK4c3QD"
      },
      "source": [
        "The ```plot_image``` function  displays a poster according to it's path.  \n",
        "Fill the ```plot_images``` function to plot a series of posters from a list of paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvuAcBau2dki"
      },
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plot_image(path):\n",
        "  img = mpimg.imread(path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "\n",
        "def plot_images(paths_list):\n",
        "  ...\n",
        "\n",
        "\n",
        "plot_images(['MLP-20M/MLP-20M/1.jpg', 'MLP-20M/MLP-20M/2.jpg', 'MLP-20M/MLP-20M/3.jpg', 'MLP-20M/MLP-20M/4.jpg', 'MLP-20M/MLP-20M/5.jpg'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXTSjpBdnti"
      },
      "source": [
        "Fill the following code to implement a function that plots the top 5 recommendations for a movie according to its index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTRaBwlR7_a6"
      },
      "outputs": [],
      "source": [
        "def plot_reco(idx, sim_matrix):\n",
        "  img = plot_image(df['path'][idx])\n",
        "  recos = ...\n",
        "  reco_posters = ...\n",
        "  plot_images(reco_posters)\n",
        "\n",
        "idx = 16 #10 #200\n",
        "plot_reco(idx, cosine_sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2j-aaICyrhv"
      },
      "source": [
        "Try with different movie indexes, you will be surprised by the lack of originality of the poster designers!  \n",
        "Look at [this post](https://thechive.com/2020/01/20/all-movie-posters-look-the-same-what-the-hell-is-this-sht-20-photos/) to convince yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BKafhI57_eB"
      },
      "source": [
        "## Vector databases  \n",
        "\n",
        "Computing the similarity matrix is a very expensive operation in terms of memory and computation time that scales quadratically with the number of items.  It is also not very practical to store this matrix in a database.  \n",
        "Vector databases offer a suite of advantages tailored to the needs of modern data analytics and machine learning applications. At their core, they specialize in handling high-dimensional data, allowing for efficient similarity searchesâ€”something traditional databases struggle with. This makes them invaluable for tasks like recommendation systems, image searches, and semantic text retrieval. Beyond search efficiency, vector databases are designed for scalability, capable of handling billions of vectors without a hitch. Furthermore, they often come with built-in mechanisms for distributed storage and computation, ensuring data resilience and fast query speeds even as data volumes grow. In essence, vector databases are the backbone that empowers many of the AI-driven solutions we see today.  \n",
        "For our current session, we're aiming for simplicity and efficiency. Many of the standard vector databases, like [Milvus](https://milvus.io/) or [Weaviate](https://weaviate.io/), require their own dedicated installations and often need to run on separate servers or containers.  \n",
        "This can introduce added complexity. Instead, we're turning to [Annoy](https://github.com/spotify/annoy) a straightforward library developped by Spotify that lets us perform efficient neighbor searches without the installation and server overhead. While Annoy is great for our present needs, do keep in mind that in larger, production scenarios, the capabilities of a dedicated vector database could be invaluable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skuNsFQF8vOf"
      },
      "outputs": [],
      "source": [
        "!pip install annoy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLnPlESj84OM"
      },
      "source": [
        "We will thus use the annoy library to create a vector database with our features.  \n",
        "In our case the database will be a simple file on the disk that we will load in memory when needed.\n",
        "To create the database, we will use an AnnoyIndex with the same dimensionality as our features and a metric to compute the distance between vectors.  \n",
        "Here we will use the cosine distance and set the database vector size to 576 (the size of our features)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpVndS-S8ybw"
      },
      "outputs": [],
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "dim = 576\n",
        "annoy_index = AnnoyIndex(dim, 'angular')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfbSRJsm9IrU"
      },
      "source": [
        "We will now fill the database with our features as indexed vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPsjj-tg9ItF"
      },
      "outputs": [],
      "source": [
        "for i, embedding in enumerate(features_list):\n",
        "    annoy_index.add_item(i, embedding)\n",
        "\n",
        "annoy_index.build(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9bB6GMAOP-"
      },
      "source": [
        "The `annoy_index.build(10)` command builds the underlying data structures required for Annoy to perform its approximate nearest neighbor searches. Specifically, the number 10 indicates that Annoy should construct 10 trees. Building multiple trees helps improve the accuracy of the search at the expense of using more memory. In essence, this command is preparing Annoy to efficiently handle future queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llsftPc4BFIT"
      },
      "source": [
        "Vector databases excel at efficient similarity searches due to their foundational design principles. Traditional databases rely on exact matches or predefined indices to retrieve data. In contrast, vector databases operate in the realm of high-dimensional vector spaces. Here, data points (or vectors) that are semantically or contextually similar are often closer in distance. By leveraging algorithms optimized for these spaces, like approximate nearest neighbor (ANN) searches, vector databases quickly identify data points that lie near a query point. This spatial awareness enables them to bypass exhaustive scans and zero in on relevant results, making similarity searches both rapid and precise.\n",
        "Thus querying the database for the most similar items to a given item is a very fast operation.  \n",
        "Complete the following function to retrieve the most similar items to a given item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkVVR0iB9iNi"
      },
      "outputs": [],
      "source": [
        "def search(query_vector, k=5):\n",
        "    indices = annoy_index.get_nns_by_vector(query_vector, k)\n",
        "    paths = ...\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHNffwc7BpBZ"
      },
      "source": [
        "Use this function to plot the top-k recommendations for a movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXpDbwJB90XU"
      },
      "outputs": [],
      "source": [
        "index = 600\n",
        "plot_image(df.path[index])\n",
        "query_vector = df.features[index]\n",
        "result = search(query_vector)\n",
        "plot_images(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save you database, you will need it for your project."
      ],
      "metadata": {
        "id": "Xh_DS7P6dOAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwiTFgWTBiO6"
      },
      "outputs": [],
      "source": [
        "annoy_index.save('rec_imdb.ann')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOsEu26KqDst"
      },
      "source": [
        "# Collaborative filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cWqD4MV6fi-"
      },
      "source": [
        "### Item-Item\n",
        "\n",
        "Item-item collaborative filtering, is a form of collaborative filtering for recommender systems based on the similarity between items calculated using people's ratings.\n",
        "For sake of simplicity, in this practical session, we will only focus on item-item similarity methods.\n",
        "If you have time, feel free to try an user-item approach. The following [blog post](https://notebook.community/saksham/recommender-systems/Collaborative%20Filtering) may help you to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Xl2hH6z5eX"
      },
      "source": [
        "We will use another dataset containing the ratings of several users on movies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCAmkwCvpX_-"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/wikistat/AI-Frameworks/master/RecomendationSystem/movielens_small/movies.csv\n",
        "!wget https://raw.githubusercontent.com/wikistat/AI-Frameworks/master/RecomendationSystem/movielens_small/ratings.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yik0HALDKnt"
      },
      "outputs": [],
      "source": [
        "ratings = pd.read_csv('ratings.csv')\n",
        "ratings = ratings.rename(columns={'movieId':'id'})\n",
        "ratings['id'] = pd.to_numeric(ratings['id'])\n",
        "ratings = pd.merge(ratings, metadata[['title', 'id']], on='id')[['userId', 'id', 'rating', 'title']]\n",
        "ratings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT5X5aV20FQ7"
      },
      "outputs": [],
      "source": [
        "ratings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Ox7f-q0IK2"
      },
      "source": [
        "This dataset is a bit huge and may slow down futur computations. Moreover collaborative filtering kind of suffers from products or user with few ratings.\n",
        "We will only focus on the 100 movies with the most ratings and the users with the highest number of ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeoIwtXZEjQi"
      },
      "outputs": [],
      "source": [
        "# filter movies\n",
        "ratings['count'] = ratings.groupby('id').transform('count')['userId']\n",
        "movieId = ratings.drop_duplicates('id').sort_values(\n",
        "    'count', ascending=False).iloc[:200]['id']\n",
        "ratings = ratings[ratings['id'].isin(movieId)].reset_index(drop=True)\n",
        "\n",
        "#filter users\n",
        "ratings['count'] = ratings.groupby('userId').transform('count')['id']\n",
        "userId = ratings.drop_duplicates('userId').sort_values(\n",
        "    'count', ascending=False).iloc[:20001]['userId']\n",
        "ratings = ratings[ratings['userId'].isin(userId)].reset_index(drop=True)\n",
        "\n",
        "ratings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3QZ374mZZKx"
      },
      "outputs": [],
      "source": [
        "ratings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCQaIr9TGtBB"
      },
      "outputs": [],
      "source": [
        "ratings.title.unique()[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egk9md1V1gUx"
      },
      "source": [
        "Now, we need to build a pivot table with user in lines, movies in columns and ratings as values.  \n",
        "Use pandas [pivot_table](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) method to create this pivot table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YcyuTdh2_9y"
      },
      "outputs": [],
      "source": [
        "pivot = ...\n",
        "pivot.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUgKWd4p2M9X"
      },
      "source": [
        "With this pivot table, it is now easy to compute the similarity between movies.\n",
        "Indeed each movie can be represented by a vector of the users' ratings.\n",
        "Instead of using a cosine similarity distance as we did earlier in the notebook, we will use the Pearson correlation score since it is already implemented in Pandas.\n",
        "The pivot table has a method [```corrwith```](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html) that will return the Pairwise correlation score of one entry with all entries of the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y-jHNCu3lyS"
      },
      "outputs": [],
      "source": [
        "movie_vector = pivot[\"Titanic\"]\n",
        "similarity = pivot.corrwith(movie_vector)\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ont3FZJw38xb"
      },
      "source": [
        "Sort the produced results to get the best recommendations to \"Titanic\".\n",
        "You may also try with different movies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUHPO2SY4TPz"
      },
      "source": [
        "## Matrix factorization\n",
        "Matrix factorization is certainly one of the most efficient way to build a recomender system. I really encourage you to have a look to [this article](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) presenting the matrix factorization techniques used in recommender systems.\n",
        "\n",
        "The idea is pretty simple, it consists in factorizing the ratings matrix $R$ into the product of a user embedding matrix $U$ and an item embedding matrix $V$, such that $R \\approx UV^\\top$ with\n",
        "$U = \\begin{bmatrix} u_{1} \\\\ \\hline \\vdots \\\\ \\hline u_{N} \\end{bmatrix}$ and\n",
        "$V = \\begin{bmatrix} v_{1} \\\\ \\hline \\vdots \\\\ \\hline v_{M} \\end{bmatrix}$.\n",
        "\n",
        "Where\n",
        "- $N$ is the number of users,\n",
        "- $M$ is the number of items,\n",
        "- $R_{ij}$ is the rating of the $j$th item by the $i$th user,\n",
        "- each row $U_i$ is a $d$-dimensional vector (embedding) representing user $i$,\n",
        "- each row $V_j$ is a $d$-dimensional vector (embedding) representing item $j$,\n",
        "\n",
        "\n",
        "One these emmbeding matrices are built, predicting the rating of an user $i$ for an item $j$ consists in computing the dot product $\\langle U_i, V_j \\rangle$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YynGarm57S4t"
      },
      "source": [
        "### Using surpise\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1dh2RJ95F0j-rZyuf59G35239B42veAWD)\n",
        "\n",
        "We will begin by using the famous Singular Value Decomposition method.\n",
        "Several libraries implement this algorithm.\n",
        "In this session, we will be using [Surprise](http://surpriselib.com/).\n",
        "Surprise is a recommender system library implemented in Python.\n",
        "It was actually developed by [Nicolas Hug](http://nicolas-hug.com/about) an INSA Toulouse Alumni!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_ZZ7J-8YHiF"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8wLyUJXPMtf"
      },
      "source": [
        "Surprise implements the SVD algorithm.  Help yourself with [the doc](https://surprise.readthedocs.io/en/stable/getting_started.html) to train an SVD model on the rating dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuPtL9WcPeKu"
      },
      "outputs": [],
      "source": [
        "#Creating a train and a test set\n",
        "testset = ratings.sample(frac=0.1, replace=False)\n",
        "trainset = ratings[~ratings.index.isin(testset.index)]\n",
        "\n",
        "assert set(testset.userId.unique()).issubset(trainset.userId.unique())\n",
        "assert set(testset.id.unique()).issubset(trainset.id.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiSBnIgdX-7U"
      },
      "outputs": [],
      "source": [
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N186i9amYSfA"
      },
      "outputs": [],
      "source": [
        "reader = ...\n",
        "data = Dataset.load_from_df(ratings[['userId', 'id', 'rating']].fillna(0), reader)\n",
        "svd = ...\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty9A91JqZLFA"
      },
      "outputs": [],
      "source": [
        "#full dataset training\n",
        "svd = SVD()\n",
        "s_trainset = data.build_full_trainset()\n",
        "svd.fit(s_trainset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFF62xdcB88T"
      },
      "source": [
        "Let us look some ratings for one user in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCcwZ4JEaGUC"
      },
      "outputs": [],
      "source": [
        "testset[testset['userId'] == 24]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVfO7wKXCDV0"
      },
      "source": [
        "What would your model predict for these exemples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sybGFavmZtT0"
      },
      "outputs": [],
      "source": [
        "uid = 24\n",
        "iid = ...\n",
        "\n",
        "svd.predict(uid, iid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWUTZcYbcIUY"
      },
      "source": [
        "Write a code to recommend 5 movies to an user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYbnpl6sQvRR"
      },
      "source": [
        "### Using gradient descent\n",
        "Another way to compute the matrix factorization consists in using gradient descent to minimize $\\text{MSE}(R, UV^\\top)$ where:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{MSE}(A, UV^\\top)\n",
        "&= \\frac{1}{|\\Omega|}\\sum_{(i, j) \\in\\Omega}{( R_{ij} - (UV^\\top)_{ij})^2} \\\\\n",
        "&= \\frac{1}{|\\Omega|}\\sum_{(i, j) \\in\\Omega}{( R_{ij} - \\langle U_i, V_j\\rangle)^2}\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\Omega$ is the set of observed ratings, and $|\\Omega|$ is the cardinality of $\\Omega$.\n",
        "\n",
        "We will now implement our own matrix factorization algorith using Pytorch.  \n",
        "To do so we first need to convert our ratings datasets in Pytorch datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES41z_Q__taJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "user_list = trainset.userId.unique()\n",
        "item_list = trainset.id.unique()\n",
        "user2id = {w: i for i, w in enumerate(user_list)}\n",
        "item2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "class Ratings_Datset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user = user2id[self.df['userId'][idx]]\n",
        "        user = torch.tensor(user, dtype=torch.long)\n",
        "        item = item2id[self.df['id'][idx]]\n",
        "        item = torch.tensor(item, dtype=torch.long)\n",
        "        rating = torch.tensor(self.df['rating'][idx], dtype=torch.float)\n",
        "        return user, item, rating\n",
        "\n",
        "\n",
        "trainloader = DataLoader(Ratings_Datset(trainset), batch_size=512, shuffle=True ,num_workers=2)\n",
        "testloader = DataLoader(Ratings_Datset(testset), batch_size=64, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIaZBIqpZIZA"
      },
      "source": [
        "These dataloader will provide mini-batches of tuples <user, movie, rating>.\n",
        "We will use a special type of Pytorch layers call [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
        "These layers will create a mapping between an index and a vector representation.\n",
        "In our case they will provide vector representations of our users and items.\n",
        "We will train the matrix factorization model to minimize the prediction error between a rating and the dot product of an user embedding with a movie embedding.\n",
        "![](https://drive.google.com/uc?export=view&id=1wSQbcSN_I28mF74-wnb8_qjAzRH9YDjA)\n",
        "\n",
        "Complete the following code to implement the ```MatrixFactorization``` class in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f1g_NCiFCLC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MatrixFactorization(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, n_factors=20):\n",
        "        super().__init__()\n",
        "        self.user_embeddings = ...\n",
        "        self.item_embeddings = ...\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        return torch.mul(self.user_embeddings(user), self.item_embeddings(item)).sum(1)\n",
        "        #return (self.user_embeddings(user)*self.item_embeddings(item)).sum(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQKw2KyztCOa"
      },
      "source": [
        "Complete the training method that we will use to train the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARev-51UGzDc"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from statistics import mean\n",
        "\n",
        "\n",
        "def train(model, optimizer, trainloader, epochs=30):\n",
        "    criterion = nn.MSELoss(reduction='mean')\n",
        "    t = tqdm(range(epochs))\n",
        "    for epoch in t:\n",
        "        total = 0\n",
        "        train_loss = []\n",
        "        for users, items, r in trainloader:\n",
        "            users = users.cuda()\n",
        "            items = items.cuda()\n",
        "            r = r.cuda() / 5\n",
        "            r = r.unsqueeze(1)\n",
        "            y_hat = ...\n",
        "            loss = ...\n",
        "            train_loss.append(loss.item())\n",
        "            total += r.size(0)\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "            t.set_description(f\"loss: {mean(train_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6s_pnyStNM-"
      },
      "source": [
        "We now have everything to train our model.\n",
        "Train your model for 5 to 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxJCX-TH8qeG"
      },
      "outputs": [],
      "source": [
        "n_user = trainset.userId.nunique()\n",
        "n_items = trainset.id.nunique()\n",
        "model = MatrixFactorization(n_user, n_items).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer, trainloader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SJN4nM8ePO"
      },
      "source": [
        "Complete the following code to evaluate your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubxaMOTHuGgi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def test(model, testloader):\n",
        "\n",
        "\n",
        "    running_mae = 0\n",
        "    with torch.no_grad():\n",
        "        total = 0\n",
        "        for users, items, r in testloader:\n",
        "            users = users.cuda()\n",
        "            items = items.cuda()\n",
        "            y = r.cuda() / 5\n",
        "            y_hat = ...\n",
        "            error = ...\n",
        "\n",
        "            running_mae += error\n",
        "            total += y.size(0)\n",
        "\n",
        "    mae = running_mae/total\n",
        "    return mae * 5\n",
        "\n",
        "\n",
        "test(model, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1WpW_n8p1K"
      },
      "source": [
        "Try to compare the predictions of your model with actual ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tny5-w5kMZuJ"
      },
      "outputs": [],
      "source": [
        "users, movies, r = next(iter(testloader))\n",
        "users = users.cuda()\n",
        "movies = movies.cuda()\n",
        "r = r.cuda()\n",
        "\n",
        "y = model(users, movies)*5\n",
        "print(\"ratings\", r[:10].data)\n",
        "print(\"predictions:\", y.flatten()[:10].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhJKjaZ_x_NQ"
      },
      "source": [
        "We just trained a matrix factorization algorithm using Pytorch.\n",
        "In this setting, the final prediction was made with the dot product of our embeddings.\n",
        "Actually with a minimal modification of the Class, we could create a full neural network.\n",
        "If we replace the dot product with a fully-connected network, we would actually have an end-to-end neural network able to predict the ratings of our users.\n",
        "![](https://drive.google.com/uc?export=view&id=1THBMB-Z3db0Rn0dyYYWhN98AHcYEM-nT)  \n",
        "This approach is called Neural Collaborative Filtering and is presented in this [paper](https://arxiv.org/pdf/1708.05031.pdf).\n",
        "Try to fill in the following code to create an NCF network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQa2cusiWbet"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "\n",
        "    def __init__(self, n_users, n_items, n_factors=8):\n",
        "        super().__init__()\n",
        "        self.user_embeddings = torch.nn.Embedding(n_users, n_factors)\n",
        "        self.item_embeddings = torch.nn.Embedding(n_items, n_factors)\n",
        "        self.predictor = torch.nn.Sequential(\n",
        "            ...\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, user, item):\n",
        "\n",
        "\n",
        "        u = self.user_embeddings(user)\n",
        "        i = self.item_embeddings(item)\n",
        "\n",
        "        # Concat the two embedding layers\n",
        "        z = torch.cat([u, i], dim=-1)\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NhrFlrT-zef"
      },
      "source": [
        "Train your NCF network on the train dataset and test it on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0dFPQrWa3bZ"
      },
      "outputs": [],
      "source": [
        "model = NCF(n_user, n_items).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, optimizer, trainloader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlfbVGeG5Hwf"
      },
      "outputs": [],
      "source": [
        "test(model, testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXWXO0ex7WZI"
      },
      "outputs": [],
      "source": [
        "users, movies, r = next(iter(testloader))\n",
        "users = users.cuda()\n",
        "movies = movies.cuda()\n",
        "r = r.cuda()\n",
        "\n",
        "y = model(users, movies)*5\n",
        "print(\"ratings\", r[:10].data)\n",
        "print(\"predictions:\", y.flatten()[:10].data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxSeYI6AqmMd"
      },
      "source": [
        "### Implicit feedback with pytorch\n",
        "\n",
        "In this practical session, we only worked with explicit feedbacks (ratings).\n",
        "Sometimes you do not have access to such quantitative feedback and have to deal with implicit feedback.\n",
        "An implicit feedback is a user's qualitative interaction with an item, such as clicking on an item (positive feedback) or stopping watching a video (negative feedback).\n",
        "If you are interested in neural collaborative filtering in the case of implicit feedback, I recommend you look at this [excellent tutorial](https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}